# ========model========
meg_encoder:  scmbm
image_encoder: vit_clip16
decoder: mapping
# ========prerpocessing========
preprocess: base

# ========dataset========
dataset_name:
    train:
        # drama: sbj_1-session_all # all 1_3, 1~3
        GOD: sbj_1-train-session_1_2_3_4_5_7_8_9_10_11
    val:
        GOD: sbj_1-train-session_6_12
dataset_yaml:
    # drama: drama/drama_vc.yaml # drama_config.yaml # region 160 ch
    GOD: GOD/god_vc.yaml # god_config.yaml
total_limit:
    train:
        GOD: 6000
        # drama: 6000 # total number of samples across sessions about: 31200
    val:
        GOD: 1200 # total number of samples across sessions

# ========path========
h5_root: ../../../dataset/ssl_dataset/{h5_name}
resume_path: null
meg_encoder_path: '../../results_ssl/{exp_name}/ckpt/best.pth' #
image_encoder_path: null # null is allowed

# ========training========
training:
    precision: 16
    val_check_interval: 10
    num_workers: 4 # 4
    batch_size: 50
    lr: 2.5e-4
    min_lr: 0
    weight_decay: 0.05
    num_epoch: 500
    warmup_epochs: 40
    # clip_grad: 0.8
    accum_iter: 1
    # mask_ratio: 0.75
    global_pool: True

    logdir: '../../results_task/god_6k_cossim_gp_lora/{exp_name}/log'
    ckpt_dir: '../../results_task/god_6k_cossim_gp_lora/{exp_name}/ckpt'
    # ========fine-tuning========
    meg_encoder_finetune: lora
    image_encoder_finetune: none
    # ========RoLA=========
    lora_config:
        r: 8
        lora_alpha: 2
        target_modules:
            - qkv
            - fc1
            - fc2
            # - proj
        lora_dropout: 0.01

    # ========criterion========
    criterion: cosin_sim
    # ========CLIP LOSS=======
    clip:
        reduction: 'mean'
        init_temperature: 5.1

