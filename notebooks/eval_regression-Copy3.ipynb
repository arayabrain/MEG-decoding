{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b05c334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, sys, random\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from time import time\n",
    "from tqdm import tqdm, trange\n",
    "from termcolor import cprint\n",
    "# import wandb\n",
    "\n",
    "from omegaconf import DictConfig, open_dict\n",
    "import hydra\n",
    "from hydra.utils import get_original_cwd\n",
    "\n",
    "from constants import device\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, BatchSampler\n",
    "\n",
    "from meg_decoding.models import get_model, Classifier\n",
    "from meg_decoding.utils.get_dataloaders import get_dataloaders, get_samplers\n",
    "from meg_decoding.utils.loss import *\n",
    "from meg_decoding.dataclass.god import GODDatasetBase, GODCollator\n",
    "from meg_decoding.utils.loggers import Pickleogger\n",
    "from meg_decoding.utils.vis_grad import get_grad\n",
    "from torch.utils.data.dataset import Subset\n",
    "from meg_ssl.dataclass import parse_dataset\n",
    "from omegaconf import OmegaConf\n",
    "from meg_ssl.models.image_encoder import get_image_encoder\n",
    "import wandb\n",
    "from meg_ssl.utils.image_preprocess import numpy2image\n",
    "import tqdm\n",
    "import scipy.io\n",
    "from hydra import initialize, compose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e203f2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(cfg:OmegaConf):\n",
    "    dataset_names:dict = cfg.dataset_name\n",
    "    dataset_yamls:dict = cfg.dataset_yaml\n",
    "    num_trial_limit:dict = cfg.total_limit\n",
    "    preproc_config:OmegaConf = cfg.preprocess\n",
    "    h5_root:str = cfg.h5_root\n",
    "    image_preprocs:list = []\n",
    "    meg_preprocs:list = []\n",
    "    only_meg:bool = False\n",
    "    on_memory:bool = True #False\n",
    "    dataset_dict:dict = parse_dataset(dataset_names, dataset_yamls, preproc_config, num_trial_limit, \n",
    "                                      h5_root, image_preprocs, meg_preprocs, only_meg, on_memory)\n",
    "    \n",
    "    return dataset_dict['train'], dataset_dict['val']\n",
    "\n",
    "def get_models(args):\n",
    "    with initialize(config_path='../meg_ssl/task_configs/model'):\n",
    "        args.image_encoder = compose(args.image_encoder)\n",
    "    with initialize(config_path='../meg_ssl/ssl_configs/preprocess'):\n",
    "        args.preprocess = compose(args.preprocess)\n",
    "    \n",
    "    image_encoder, image_processor = get_image_encoder(args.image_encoder.name, args.image_encoder.parameters)\n",
    "    image_encoder.eval().to(device)\n",
    "    \n",
    "    brain_encoder = get_model(args).to(device).eval() \n",
    "    \n",
    "    weight_path = os.path.join(args.save_root, 'weights', 'model_last.pt')\n",
    "    brain_encoder.load_state_dict(torch.load(weight_path))\n",
    "    return image_encoder, image_processor, brain_encoder\n",
    "    \n",
    "# collect image feature\n",
    "def get_features(dataset):\n",
    "    image_features = []\n",
    "    eeg_features = []\n",
    "    for data in tqdm.tqdm(dataset):\n",
    "        eeg, image = data\n",
    "        eeg = torch.from_numpy(eeg).unsqueeze(0).to(device)\n",
    "        image = numpy2image(image)\n",
    "        image = image_processor(image)\n",
    "        image = image.unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            Y = image_encoder.encode_image(image).to(torch.float)\n",
    "            Y = Y - normalize_mean_Y\n",
    "            Y = Y / normalize_std_Y\n",
    "            Z = brain_encoder(eeg, None)\n",
    "        image_features.append(Y)\n",
    "        eeg_features.append(Z)\n",
    "\n",
    "    image_features = torch.cat(image_features, axis=0)\n",
    "    eeg_features = torch.cat(eeg_features, axis=0)\n",
    "    return image_features, eeg_features\n",
    "\n",
    "\n",
    "def calc_similarity(x, y):\n",
    "    batch_size = len(x)\n",
    "    gt_size = len(y)\n",
    "\n",
    "    similarity = torch.empty(batch_size, gt_size).to('cuda')\n",
    "    for i in range(batch_size):\n",
    "        for j in range(gt_size):\n",
    "            similarity[i, j] = (x[i] @ y[j]) / max((x[i].norm() * y[j].norm()), 1e-8)\n",
    "    return similarity.cpu().numpy()\n",
    "\n",
    "def evaluate(Z, Y, index = None):\n",
    "    # Z: (batch_size, 512)\n",
    "    # Y: (gt_size, 512)\n",
    "    binary_confusion_matrix = np.zeros([len(Z), len(Y)])\n",
    "    similarity = calc_similarity(Z, Y)\n",
    "    acc_tmp = np.zeros(len(similarity))\n",
    "    for i in range(len(similarity)):\n",
    "        if index is None:\n",
    "            index_ = i\n",
    "        acc_tmp[i] = np.sum(similarity[i,:] < similarity[i,index_]) / (len(Y)-1)\n",
    "        binary_confusion_matrix[i,similarity[i,:] < similarity[i,index_]] = 1 \n",
    "        binary_confusion_matrix[i,similarity[i,:] > similarity[i,index_]] = -1 \n",
    "    similarity_acc = np.mean(acc_tmp)\n",
    "    print('Similarity Acc', similarity_acc)\n",
    "    return similarity_acc, binary_confusion_matrix\n",
    "\n",
    "def get_test_labels():\n",
    "    dirname = '/work/project/MEG_GOD/GOD_dataset/sbj01/labels/'\n",
    "    labels = []\n",
    "    for i in range(1,7):\n",
    "        filepath = os.path.join(dirname, f'val_{i}.mat')\n",
    "        matdata = scipy.io.loadmat(filepath)\n",
    "        label = matdata['vec_index'][0] \n",
    "        labels.append(label)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def aggregate_same_label(eegs, images, labels):\n",
    "    assert len(eegs) == len(labels)\n",
    "    assert len(images) == len(labels)\n",
    "    unique_label_ids = np.unique(labels)\n",
    "    new_eegs = []\n",
    "    new_images = []\n",
    "    for i in unique_label_ids:\n",
    "        indices = np.where(labels==i)\n",
    "        new_eegs.append(eegs[indices].mean(0))\n",
    "        new_images.append(images[indices].mean(0))\n",
    "    return torch.stack(new_eegs, axis=0), torch.stack(new_images, axis=0), unique_label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bbbcf03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_54970/3832526577.py:17: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  with initialize(config_path='../meg_ssl/task_configs/model'):\n",
      "/tmp/ipykernel_54970/3832526577.py:19: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  with initialize(config_path='../meg_ssl/ssl_configs/preprocess'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model select:  eegnet_deep\n",
      "ROI:  ['occipital/left', 'occipital/right', 'frontal/left', 'frontal/right', 'temporal/left', 'temporal/right', 'parietal/left', 'parietal/right', 'central/left', 'central/right']\n",
      "channel (-1 is done because matlab starts from 1):  [128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 38, 43, 45, 46, 47, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 78, 87, 106, 108, 109, 110, 111, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 44, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 61, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 112, 113, 114, 115, 117, 118, 119, 120, 121, 123, 125, 17, 19, 20, 21, 22, 23, 29, 59, 60, 63, 139, 140, 141, 142, 143, 80, 81, 82, 85, 86, 88, 116, 124, 126, 155, 156, 157, 159, 12, 13, 14, 15, 16, 18, 24, 25, 26, 27, 28, 30, 31, 58, 62, 158, 74, 77, 79, 83, 84, 89, 90, 91, 92, 93, 94, 95, 107, 122, 127]\n",
      "num channels:  160\n",
      "=================GOD=================\n",
      "sbj_1-train-session_6_12\n",
      "dataset_info_list:  [{'image_root': '/storage/dataset/ECoG/internal/GODv2-4/images_trn', 'meg_path': '/work/project/MEG_GOD/GOD_dataset/sbj01/mat/data_block006', 'meg_label_path': '/work/project/MEG_GOD/GOD_dataset/sbj01/labels/ses_6', 'meg_trigger_path': '/work/project/MEG_GOD/GOD_dataset/sbj01/trigger/ses06', 'meg_rest_path': '/work/project/MEG_GOD/GOD_dataset/sbj01/mat/data_block006', 'sbj_name': 'sbj01', 'h5_file_name': '../../../dataset/ssl_dataset/sbj1/regression/GOD/sbj01_data_block006.h5', 'image_id_path': '/work/project/MEG_GOD/GOD_dataset/clip_image_training.mat'}, {'image_root': '/storage/dataset/ECoG/internal/GODv2-4/images_trn', 'meg_path': '/work/project/MEG_GOD/GOD_dataset/sbj01/mat/data_block012', 'meg_label_path': '/work/project/MEG_GOD/GOD_dataset/sbj01/labels/ses_12', 'meg_trigger_path': '/work/project/MEG_GOD/GOD_dataset/sbj01/trigger/ses12', 'meg_rest_path': '/work/project/MEG_GOD/GOD_dataset/sbj01/mat/data_block012', 'sbj_name': 'sbj01', 'h5_file_name': '../../../dataset/ssl_dataset/sbj1/regression/GOD/sbj01_data_block012.h5', 'image_id_path': '/work/project/MEG_GOD/GOD_dataset/clip_image_training.mat'}]\n",
      "=====================================\n",
      "ROI:  ['occipital/left', 'occipital/right', 'frontal/left', 'frontal/right', 'temporal/left', 'temporal/right', 'parietal/left', 'parietal/right', 'central/left', 'central/right']\n",
      "channel (-1 is done because matlab starts from 1):  [128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 38, 43, 45, 46, 47, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 78, 87, 106, 108, 109, 110, 111, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 44, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 61, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 112, 113, 114, 115, 117, 118, 119, 120, 121, 123, 125, 17, 19, 20, 21, 22, 23, 29, 59, 60, 63, 139, 140, 141, 142, 143, 80, 81, 82, 85, 86, 88, 116, 124, 126, 155, 156, 157, 159, 12, 13, 14, 15, 16, 18, 24, 25, 26, 27, 28, 30, 31, 58, 62, 158, 74, 77, 79, 83, 84, 89, 90, 91, 92, 93, 94, 95, 107, 122, 127]\n",
      "num channels:  160\n",
      "band path filter: 0.5-120\n",
      "resample 1000 to 240 Hz\n",
      "ROI:  ['occipital/left', 'occipital/right', 'frontal/left', 'frontal/right', 'temporal/left', 'temporal/right', 'parietal/left', 'parietal/right', 'central/left', 'central/right']\n",
      "channel (-1 is done because matlab starts from 1):  [128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 38, 43, 45, 46, 47, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 78, 87, 106, 108, 109, 110, 111, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 44, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 61, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 112, 113, 114, 115, 117, 118, 119, 120, 121, 123, 125, 17, 19, 20, 21, 22, 23, 29, 59, 60, 63, 139, 140, 141, 142, 143, 80, 81, 82, 85, 86, 88, 116, 124, 126, 155, 156, 157, 159, 12, 13, 14, 15, 16, 18, 24, 25, 26, 27, 28, 30, 31, 58, 62, 158, 74, 77, 79, 83, 84, 89, 90, 91, 92, 93, 94, 95, 107, 122, 127]\n",
      "num channels:  160\n",
      "band path filter: 0.5-120\n",
      "resample 1000 to 240 Hz\n",
      "=================GOD=================\n",
      "sbj_1-val-session_all\n",
      "dataset_info_list:  [{'image_root': '/storage/dataset/ECoG/internal/GODv2-4/images_val', 'meg_path': '/work/project/MEG_GOD/GOD_dataset/sbj01/mat/data_val001', 'meg_label_path': '/work/project/MEG_GOD/GOD_dataset/sbj01/labels/val_1', 'meg_trigger_path': '/work/project/MEG_GOD/GOD_dataset/sbj01/trigger/val01', 'meg_rest_path': '/work/project/MEG_GOD/GOD_dataset/sbj01/mat/data_val001', 'sbj_name': 'sbj01', 'h5_file_name': '../../../dataset/ssl_dataset/sbj1/regression/GOD/sbj01_data_val001.h5', 'image_id_path': '/work/project/MEG_GOD/GOD_dataset/clip_image_test.mat'}, {'image_root': '/storage/dataset/ECoG/internal/GODv2-4/images_val', 'meg_path': '/work/project/MEG_GOD/GOD_dataset/sbj01/mat/data_val002', 'meg_label_path': '/work/project/MEG_GOD/GOD_dataset/sbj01/labels/val_2', 'meg_trigger_path': '/work/project/MEG_GOD/GOD_dataset/sbj01/trigger/val02', 'meg_rest_path': '/work/project/MEG_GOD/GOD_dataset/sbj01/mat/data_val002', 'sbj_name': 'sbj01', 'h5_file_name': '../../../dataset/ssl_dataset/sbj1/regression/GOD/sbj01_data_val002.h5', 'image_id_path': '/work/project/MEG_GOD/GOD_dataset/clip_image_test.mat'}, {'image_root': '/storage/dataset/ECoG/internal/GODv2-4/images_val', 'meg_path': '/work/project/MEG_GOD/GOD_dataset/sbj01/mat/data_val003', 'meg_label_path': '/work/project/MEG_GOD/GOD_dataset/sbj01/labels/val_3', 'meg_trigger_path': '/work/project/MEG_GOD/GOD_dataset/sbj01/trigger/val03', 'meg_rest_path': '/work/project/MEG_GOD/GOD_dataset/sbj01/mat/data_val003', 'sbj_name': 'sbj01', 'h5_file_name': '../../../dataset/ssl_dataset/sbj1/regression/GOD/sbj01_data_val003.h5', 'image_id_path': '/work/project/MEG_GOD/GOD_dataset/clip_image_test.mat'}, {'image_root': '/storage/dataset/ECoG/internal/GODv2-4/images_val', 'meg_path': '/work/project/MEG_GOD/GOD_dataset/sbj01/mat/data_val004', 'meg_label_path': '/work/project/MEG_GOD/GOD_dataset/sbj01/labels/val_4', 'meg_trigger_path': '/work/project/MEG_GOD/GOD_dataset/sbj01/trigger/val04', 'meg_rest_path': '/work/project/MEG_GOD/GOD_dataset/sbj01/mat/data_val004', 'sbj_name': 'sbj01', 'h5_file_name': '../../../dataset/ssl_dataset/sbj1/regression/GOD/sbj01_data_val004.h5', 'image_id_path': '/work/project/MEG_GOD/GOD_dataset/clip_image_test.mat'}, {'image_root': '/storage/dataset/ECoG/internal/GODv2-4/images_val', 'meg_path': '/work/project/MEG_GOD/GOD_dataset/sbj01/mat/data_val005', 'meg_label_path': '/work/project/MEG_GOD/GOD_dataset/sbj01/labels/val_5', 'meg_trigger_path': '/work/project/MEG_GOD/GOD_dataset/sbj01/trigger/val05', 'meg_rest_path': '/work/project/MEG_GOD/GOD_dataset/sbj01/mat/data_val005', 'sbj_name': 'sbj01', 'h5_file_name': '../../../dataset/ssl_dataset/sbj1/regression/GOD/sbj01_data_val005.h5', 'image_id_path': '/work/project/MEG_GOD/GOD_dataset/clip_image_test.mat'}, {'image_root': '/storage/dataset/ECoG/internal/GODv2-4/images_val', 'meg_path': '/work/project/MEG_GOD/GOD_dataset/sbj01/mat/data_val006', 'meg_label_path': '/work/project/MEG_GOD/GOD_dataset/sbj01/labels/val_6', 'meg_trigger_path': '/work/project/MEG_GOD/GOD_dataset/sbj01/trigger/val06', 'meg_rest_path': '/work/project/MEG_GOD/GOD_dataset/sbj01/mat/data_val006', 'sbj_name': 'sbj01', 'h5_file_name': '../../../dataset/ssl_dataset/sbj1/regression/GOD/sbj01_data_val006.h5', 'image_id_path': '/work/project/MEG_GOD/GOD_dataset/clip_image_test.mat'}]\n",
      "=====================================\n",
      "ROI:  ['occipital/left', 'occipital/right', 'frontal/left', 'frontal/right', 'temporal/left', 'temporal/right', 'parietal/left', 'parietal/right', 'central/left', 'central/right']\n",
      "channel (-1 is done because matlab starts from 1):  [128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 38, 43, 45, 46, 47, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 78, 87, 106, 108, 109, 110, 111, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 44, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 61, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 112, 113, 114, 115, 117, 118, 119, 120, 121, 123, 125, 17, 19, 20, 21, 22, 23, 29, 59, 60, 63, 139, 140, 141, 142, 143, 80, 81, 82, 85, 86, 88, 116, 124, 126, 155, 156, 157, 159, 12, 13, 14, 15, 16, 18, 24, 25, 26, 27, 28, 30, 31, 58, 62, 158, 74, 77, 79, 83, 84, 89, 90, 91, 92, 93, 94, 95, 107, 122, 127]\n",
      "num channels:  160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "band path filter: 0.5-120\n",
      "resample 1000 to 240 Hz\n",
      "ROI:  ['occipital/left', 'occipital/right', 'frontal/left', 'frontal/right', 'temporal/left', 'temporal/right', 'parietal/left', 'parietal/right', 'central/left', 'central/right']\n",
      "channel (-1 is done because matlab starts from 1):  [128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 38, 43, 45, 46, 47, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 78, 87, 106, 108, 109, 110, 111, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 44, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 61, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 112, 113, 114, 115, 117, 118, 119, 120, 121, 123, 125, 17, 19, 20, 21, 22, 23, 29, 59, 60, 63, 139, 140, 141, 142, 143, 80, 81, 82, 85, 86, 88, 116, 124, 126, 155, 156, 157, 159, 12, 13, 14, 15, 16, 18, 24, 25, 26, 27, 28, 30, 31, 58, 62, 158, 74, 77, 79, 83, 84, 89, 90, 91, 92, 93, 94, 95, 107, 122, 127]\n",
      "num channels:  160\n",
      "band path filter: 0.5-120\n",
      "resample 1000 to 240 Hz\n",
      "ROI:  ['occipital/left', 'occipital/right', 'frontal/left', 'frontal/right', 'temporal/left', 'temporal/right', 'parietal/left', 'parietal/right', 'central/left', 'central/right']\n",
      "channel (-1 is done because matlab starts from 1):  [128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 38, 43, 45, 46, 47, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 78, 87, 106, 108, 109, 110, 111, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 44, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 61, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 112, 113, 114, 115, 117, 118, 119, 120, 121, 123, 125, 17, 19, 20, 21, 22, 23, 29, 59, 60, 63, 139, 140, 141, 142, 143, 80, 81, 82, 85, 86, 88, 116, 124, 126, 155, 156, 157, 159, 12, 13, 14, 15, 16, 18, 24, 25, 26, 27, 28, 30, 31, 58, 62, 158, 74, 77, 79, 83, 84, 89, 90, 91, 92, 93, 94, 95, 107, 122, 127]\n",
      "num channels:  160\n",
      "band path filter: 0.5-120\n",
      "resample 1000 to 240 Hz\n",
      "ROI:  ['occipital/left', 'occipital/right', 'frontal/left', 'frontal/right', 'temporal/left', 'temporal/right', 'parietal/left', 'parietal/right', 'central/left', 'central/right']\n",
      "channel (-1 is done because matlab starts from 1):  [128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 38, 43, 45, 46, 47, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 78, 87, 106, 108, 109, 110, 111, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 44, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 61, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 112, 113, 114, 115, 117, 118, 119, 120, 121, 123, 125, 17, 19, 20, 21, 22, 23, 29, 59, 60, 63, 139, 140, 141, 142, 143, 80, 81, 82, 85, 86, 88, 116, 124, 126, 155, 156, 157, 159, 12, 13, 14, 15, 16, 18, 24, 25, 26, 27, 28, 30, 31, 58, 62, 158, 74, 77, 79, 83, 84, 89, 90, 91, 92, 93, 94, 95, 107, 122, 127]\n",
      "num channels:  160\n",
      "band path filter: 0.5-120\n",
      "resample 1000 to 240 Hz\n",
      "ROI:  ['occipital/left', 'occipital/right', 'frontal/left', 'frontal/right', 'temporal/left', 'temporal/right', 'parietal/left', 'parietal/right', 'central/left', 'central/right']\n",
      "channel (-1 is done because matlab starts from 1):  [128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 38, 43, 45, 46, 47, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 78, 87, 106, 108, 109, 110, 111, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 44, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 61, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 112, 113, 114, 115, 117, 118, 119, 120, 121, 123, 125, 17, 19, 20, 21, 22, 23, 29, 59, 60, 63, 139, 140, 141, 142, 143, 80, 81, 82, 85, 86, 88, 116, 124, 126, 155, 156, 157, 159, 12, 13, 14, 15, 16, 18, 24, 25, 26, 27, 28, 30, 31, 58, 62, 158, 74, 77, 79, 83, 84, 89, 90, 91, 92, 93, 94, 95, 107, 122, 127]\n",
      "num channels:  160\n",
      "band path filter: 0.5-120\n",
      "resample 1000 to 240 Hz\n",
      "ROI:  ['occipital/left', 'occipital/right', 'frontal/left', 'frontal/right', 'temporal/left', 'temporal/right', 'parietal/left', 'parietal/right', 'central/left', 'central/right']\n",
      "channel (-1 is done because matlab starts from 1):  [128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 38, 43, 45, 46, 47, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 78, 87, 106, 108, 109, 110, 111, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 44, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 61, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 112, 113, 114, 115, 117, 118, 119, 120, 121, 123, 125, 17, 19, 20, 21, 22, 23, 29, 59, 60, 63, 139, 140, 141, 142, 143, 80, 81, 82, 85, 86, 88, 116, 124, 126, 155, 156, 157, 159, 12, 13, 14, 15, 16, 18, 24, 25, 26, 27, 28, 30, 31, 58, 62, 158, 74, 77, 79, 83, 84, 89, 90, 91, 92, 93, 94, 95, 107, 122, 127]\n",
      "num channels:  160\n",
      "band path filter: 0.5-120\n",
      "resample 1000 to 240 Hz\n"
     ]
    }
   ],
   "source": [
    "with initialize(version_base=None, config_path=\"../meg_ssl/task_configs/\"):\n",
    "    args = compose(config_name='regression_eegnet_deep_mse_full')\n",
    "    \n",
    "args.dataset_name.train = args.dataset_name.val\n",
    "args.dataset_name.val.GOD = 'sbj_1-val-session_all' # args.dataset_name.val.GOD.replace('train', 'val')\n",
    "args.ch_region_path = args.ch_region_path.replace('./', '../')\n",
    "args.montage_path = args.montage_path.replace('./', '../')\n",
    "\n",
    "if not os.path.exists(os.path.join(args.save_root, 'eval_results')):\n",
    "    os.makedirs(os.path.join(args.save_root, 'eval_results'))\n",
    "    \n",
    "    \n",
    "normalize_mean_X = torch.from_numpy(np.load('/home/yainoue/meg2image/codes/MEG-decoding/data/GOD/stats/mean_X.npy')).to(device)\n",
    "normalize_mean_Y = torch.from_numpy(np.load('/home/yainoue/meg2image/codes/MEG-decoding/data/GOD/stats/mean_Y.npy')).to(device)\n",
    "normalize_std_X = torch.from_numpy(np.load('/home/yainoue/meg2image/codes/MEG-decoding/data/GOD/stats/std_X.npy')).to(device)\n",
    "normalize_std_Y = torch.from_numpy(np.load('/home/yainoue/meg2image/codes/MEG-decoding/data/GOD/stats/std_Y.npy')).to(device)\n",
    "\n",
    "classifier = Classifier(args)\n",
    "\n",
    "# ---------------\n",
    "#      Loss\n",
    "# ---------------\n",
    "loss_func = CLIPLoss(args).to(device) # torch.nn.MSELoss(reduction=\"mean\") #CLIPLoss(args).to(device)\n",
    "loss_func.eval()\n",
    "\n",
    "image_encoder, image_processor, brain_encoder = get_models(args)\n",
    "val_dataset, test_dataset = get_dataset(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f3e0230",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [03:24<00:00,  5.88it/s]\n",
      "100%|██████████| 300/300 [00:47<00:00,  6.31it/s]\n"
     ]
    }
   ],
   "source": [
    "val_image_features, val_eeg_features = get_features(val_dataset)\n",
    "test_image_features, test_eeg_features = get_features(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71f7b703",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = get_test_labels()\n",
    "test_eeg_features_avg, test_image_features_avg, new_labels = aggregate_same_label(test_eeg_features, test_image_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23aaf473",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_mean_Y = torch.from_numpy(np.load('/home/yainoue/meg2image/codes/MEG-decoding/data/GOD/stats/mean_Y.npy')).to(device)\n",
    "normalize_mean_Y.shape\n",
    "\n",
    "normalize_mean_X = val_eeg_features.mean(0, keepdim=True).cpu().numpy()\n",
    "normalize_std_X = val_eeg_features.std(0, keepdim=True).cpu().numpy()\n",
    "\n",
    "np.save('/home/yainoue/meg2image/codes/MEG-decoding/data/GOD/stats/mean_X.npy', normalize_mean_X)\n",
    "np.save('/home/yainoue/meg2image/codes/MEG-decoding/data/GOD/stats/std_X.npy', normalize_std_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3981072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_mean_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e80746c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Similarities]: 100%|██████████| 1200/1200 [04:57<00:00,  4.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity torch.Size([1200, 1200])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Similarities]: 100%|██████████| 300/300 [00:12<00:00, 24.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity torch.Size([300, 300])\n",
      "top1: val0.00167(0.00083) test 0.01000(0.00333)\n",
      "top10: val 0.01250 test 0.04000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "valTop1acc, valTop10acc = classifier(val_eeg_features, val_image_features, test=True)\n",
    "testTop1acc, testTop10acc = classifier(test_eeg_features, test_image_features, test=True)\n",
    "val_random = 1 / len(val_eeg_features)\n",
    "test_random = 1 / len(test_eeg_features)\n",
    "print(f'top1: val{valTop1acc :.5f}({val_random :.5f}) test {testTop1acc :.5f}({test_random :.5f})')\n",
    "print(f'top10: val {valTop10acc :.5f} test {testTop10acc :.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75301f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Acc 0.5066319154851264\n",
      "Similarity Acc 0.5027201783723522\n",
      "object identification: val 0.5066319154851264 test 0.5027201783723522\n"
     ]
    }
   ],
   "source": [
    "val_sim, val_sim_mat = evaluate(val_eeg_features, val_image_features)\n",
    "test_sim, test_sim_mat = evaluate(test_eeg_features, test_image_features)\n",
    "print(f'object identification: val {val_sim} test {test_sim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a880827",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Similarities]: 100%|██████████| 50/50 [00:00<00:00, 144.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity torch.Size([50, 50])\n",
      "Similarity Acc 0.4983673469387756\n",
      "AVG: \n",
      "Top1:  0.00000, Top10: 0.30000 Acc: 0.49837\n"
     ]
    }
   ],
   "source": [
    "testAvgTop1acc, testAvgTop10acc = classifier(test_eeg_features_avg, test_image_features_avg, test=True)\n",
    "testAvg_sim, testAvg_sim_mat = evaluate(test_eeg_features_avg, test_image_features_avg)\n",
    "print('AVG: ')\n",
    "print(f'Top1: {testAvgTop1acc: .5f}, Top10: {testAvgTop10acc:.5f} Acc: {testAvg_sim:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1edd0622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "test_eeg_features_avg2 = (test_eeg_features_avg - test_eeg_features_avg.mean(0))/ test_eeg_features_avg.std(0)\n",
    "test_image_features_avg2 = (test_image_features_avg - test_image_features_avg.mean(0)) / test_image_features_avg.std(0)\n",
    "\n",
    "# print(test_eeg_features_avg2.std(0))\n",
    "# print(test_image_features_avg2.std(0))\n",
    "plt.plot(np.arange(len(test_eeg_features_avg2[0])), test_eeg_features_avg2.std(0).cpu())\n",
    "plt.plot(np.arange(len(test_image_features_avg2[0])), test_image_features_avg2.std(0).cpu())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e27096c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Acc 0.5812244897959183\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5812244897959183,\n",
       " array([[ 0., -1.,  1., ...,  1.,  1.,  1.],\n",
       "        [-1.,  0., -1., ...,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  0., ...,  1.,  1.,  1.],\n",
       "        ...,\n",
       "        [-1., -1., -1., ...,  0.,  1.,  1.],\n",
       "        [-1.,  1., -1., ...,  1.,  0.,  1.],\n",
       "        [ 1.,  1.,  1., ..., -1., -1.,  0.]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(test_eeg_features_avg2, test_image_features_avg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0be5acc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Similarities]: 100%|██████████| 50/50 [00:00<00:00, 140.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity torch.Size([50, 50])\n",
      "Top1:  0.08000, Top10: 0.30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "testAvgTop1acc, testAvgTop10acc = classifier(test_eeg_features_avg2, test_image_features_avg2, test=True)\n",
    "print(f'Top1: {testAvgTop1acc: .5f}, Top10: {testAvgTop10acc:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31916925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  tensor(-0.0061, device='cuda:0') tensor(-0.0110, device='cuda:0')\n",
      "std:  tensor(1.0513, device='cuda:0') tensor(0.0929, device='cuda:0')\n",
      "Similarity Acc 0.5481899043806981\n",
      "[[ 0. -1.  1. ...  1.  1. -1.]\n",
      " [ 1.  0.  1. ...  1.  1.  1.]\n",
      " [ 1.  1.  0. ...  1.  1.  1.]\n",
      " ...\n",
      " [-1.  1.  1. ... -1.  1. -1.]\n",
      " [-1. -1. -1. ... -1. -1. -1.]\n",
      " [ 1.  1. -1. ...  1.  1.  1.]]\n",
      "[ 0. -1.  1. ...  1.  1. -1.]\n",
      "acc 0.5481899043806981\n",
      "[ 666   52  446  547 1378 1045  863 1151 1206 1234   70  993  751  847\n",
      "  410  246   92  620  886  543  511 1205 1312 1259  707  257  937  651\n",
      "  619   69 1154  673  470  300  535 1211  720  225 1327  321   75   49\n",
      "  521 1259   53   95  764 1029  310 1476  645  501  283  485  871 1243\n",
      "  630  191  253  747  675   19  588  733  150 1276 1008 1237 1199  193\n",
      "  156  362 1492  502  470  521   26  744  516  963  749  665  796  353\n",
      "  436  526 1481 1265 1214 1488  276  219  160  837  951 1390  626  815\n",
      "  882 1423   44  136  233  426 1467 1220  535 1282 1074 1093  631  894\n",
      "  333  666 1112  552  653 1224  500    3   28  284 1440 1270  993  240\n",
      "  435  182  869 1210    7  112 1041  114 1493  871   14  607  445 1346\n",
      "  988 1100  301 1446  915  831 1493  766  250  837  370   54  122   54\n",
      "   88   18 1093  924 1352  330  901   39   10  426 1067  293 1471  641\n",
      " 1086  414 1478  193  349 1275  153 1005  737  166   52  317 1429  498\n",
      "  156  514  308  265  888  264 1393 1239  927  352  873  607 1284  387\n",
      "   11  642  169  288 1043 1406 1059  232  540 1104  744  160 1228  849\n",
      "   78  826  285   37 1209  509 1279  729  477 1488  957   52   82  475\n",
      "  377  341 1336 1349  933  640  254  710  947 1221 1411 1120 1407 1095\n",
      " 1014  207  860  703 1319  226  475  347  180 1218  459   26  496  842\n",
      "  615  558  161 1275  762  307  465  215  237  633  802  979   29  553\n",
      " 1496   72 1137  186   32  298  684 1299  457    5   24 1354  447 1427\n",
      " 1481  659  662  529  984  302   23  515 1408  870  563  387 1317  634\n",
      "   26 1090  901  399  845  365]\n",
      "top1:  0.0\n",
      "top5:  0.0033333333333333335\n",
      "top10:  0.01\n"
     ]
    }
   ],
   "source": [
    "unit_corr = []\n",
    "for i in range(val_image_features.shape[1]):\n",
    "    corr = np.corrcoef(val_image_features[:,i].cpu(), val_eeg_features[:,i].cpu())[0,1]\n",
    "    unit_corr.append(corr)\n",
    "\n",
    "unit_indices = np.argsort(unit_corr)[::-1][:100]\n",
    "\n",
    "target_latent = torch.cat([test_image_features, val_image_features], axis=0)\n",
    "target_latent_std =  target_latent.std(0)\n",
    "target_latent_mean = target_latent.mean(0)\n",
    "\n",
    "source_latent = test_eeg_features\n",
    "source_latent_std = source_latent.std(0)\n",
    "source_latent_mean = source_latent.mean(0)\n",
    "source_latent = (source_latent - source_latent_mean)/source_latent_std\n",
    "source_latent = (source_latent * target_latent_std + target_latent_mean)\n",
    "print('mean: ', target_latent_mean.mean(), source_latent_mean.mean())\n",
    "print('std: ', target_latent_std.mean(), source_latent_std.mean())\n",
    "\n",
    "source_latent = torch.from_numpy(source_latent.cpu().numpy()[:, unit_indices])# torch.from_numpy()\n",
    "target_latent = torch.from_numpy(target_latent.cpu().numpy()[:,unit_indices])\n",
    "acc, mat = evaluate(source_latent, target_latent)\n",
    "print(mat)\n",
    "print(mat[0])\n",
    "print('acc', np.mean(np.sum(mat > 0, 1) / (len(target_latent)-1)))\n",
    "print(np.sum(mat < 0, 1))\n",
    "print('top1: ',  np.mean(np.sum(mat < 0, 1) < 1))\n",
    "print('top5: ', np.mean(np.sum(mat < 0, 1) < 5))\n",
    "print('top10: ', np.mean(np.sum(mat < 0, 1) < 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac50e249",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
