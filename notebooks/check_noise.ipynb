{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 予測したlatentと実際のlatentの誤差の分布を見る\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import DictConfig, open_dict\n",
    "from meg_decoding.models import get_model, Classifier\n",
    "from meg_decoding.utils.get_dataloaders import get_dataloaders, get_samplers\n",
    "from meg_decoding.utils.loss import *\n",
    "from meg_decoding.dataclass.god import GODDatasetBase, GODCollator\n",
    "from meg_decoding.utils.loggers import Pickleogger\n",
    "# from meg_decoding.clip_utils.get_embedding import get_language_model\n",
    "from torch.utils.data.dataset import Subset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import os\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(args):\n",
    "    source_dataset = GODDatasetBase(args, 'train', return_label=True)\n",
    "    outlier_dataset = GODDatasetBase(args, 'val', return_label=True,\n",
    "                                        mean_X= source_dataset.mean_X,\n",
    "                                        mean_Y=source_dataset.mean_Y,\n",
    "                                        std_X=source_dataset.std_X,\n",
    "                                        std_Y=source_dataset.std_Y\n",
    "                                    )\n",
    "    ind_tr = list(range(0, 3000)) + list(range(3600, 6600)) #+ list(range(7200, 21600)) # + list(range(7200, 13200)) + list(range(14400, 20400))\n",
    "    ind_te = list(range(3000,3600)) + list(range(6600, 7200)) # + list(range(13200, 14400)) + list(range(20400, 21600))\n",
    "    train_dataset = Subset(source_dataset, ind_tr)\n",
    "    val_dataset   = Subset(source_dataset, ind_te)\n",
    "    train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size= args.batch_size,\n",
    "            drop_last=True,\n",
    "            shuffle=False,\n",
    "            num_workers=args.num_workers,\n",
    "            pin_memory=True,\n",
    "            worker_init_fn=seed_worker,\n",
    "            generator=g,\n",
    "        )\n",
    "    test_loader = DataLoader(\n",
    "            # val_dataset, #\n",
    "            outlier_dataset,  # val_dataset\n",
    "            batch_size=50, # args.batch_size,\n",
    "            drop_last=True,\n",
    "            shuffle=False,\n",
    "            num_workers=args.num_workers,\n",
    "            pin_memory=True,\n",
    "            worker_init_fn=seed_worker,\n",
    "            generator=g,\n",
    "        )\n",
    "    brain_encoder = get_model(args).to(device) #BrainEncoder(args).to(device)\n",
    "\n",
    "    weight_dir = os.path.join(os.path.join('/',*args.save_root.split('/')[:-2]), 'weights')\n",
    "    last_weight_file = os.path.join(weight_dir, \"model_last.pt\")\n",
    "    best_weight_file = os.path.join(weight_dir, \"model_best.pt\")\n",
    "    if os.path.exists(best_weight_file):\n",
    "        brain_encoder.load_state_dict(torch.load(best_weight_file))\n",
    "        print('weight is loaded from ', best_weight_file)\n",
    "    else:\n",
    "        brain_encoder.load_state_dict(torch.load(last_weight_file))\n",
    "        print('weight is loaded from ', last_weight_file)\n",
    "\n",
    "\n",
    "    classifier = Classifier(args)\n",
    "    \n",
    "    Zs = []\n",
    "    Ys = []\n",
    "    Ls = []\n",
    "    brain_encoder.eval()\n",
    "    for batch in test_loader:\n",
    "        with torch.no_grad():\n",
    "\n",
    "            if len(batch) == 3:\n",
    "                X, Y, subject_idxs = batch\n",
    "            elif len(batch) == 4:\n",
    "                X, Y, subject_idxs, Labels = batch\n",
    "            else:\n",
    "                raise ValueError(\"Unexpected number of items from dataloader.\")\n",
    "\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "\n",
    "            Z = brain_encoder(X, subject_idxs)  # 0.96 GB\n",
    "            Zs.append(Z)\n",
    "            Ys.append(Y)\n",
    "            Ls.append(Labels)\n",
    "\n",
    "            testTop1acc, testTop10acc = classifier(Z, Y, test=True)  # ( 250, 1024, 360 )\n",
    "\n",
    "    Zs = torch.cat(Zs, dim=0)\n",
    "    Ys = torch.cat(Ys, dim=0)\n",
    "    Ls = torch.cat(Ls, dim=0).detach().cpu().numpy()\n",
    "    raw_Es = Zs-Ys\n",
    "    \n",
    "    Zs = Zs - Zs.mean(dim=0, keepdims=True)\n",
    "    Zs = Zs / Zs.std(dim=0, keepdims=True)\n",
    "    Zs = Zs - Zs.mean(dim=1, keepdims=True)\n",
    "    Zs = Zs / Zs.std(dim=1, keepdims=True)\n",
    "    Ys = Ys - Ys.mean(dim=1, keepdims=True)\n",
    "    Ys = Ys / Ys.std(dim=1, keepdims=True)\n",
    "    \n",
    "    normalized_Es  = Zs-Ys\n",
    "    \n",
    "    fig, axes = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "    axes[0].hist(raw_Es.flatten(), bins=100)\n",
    "    axes[1].hist(normalized_Es.flatten(), bins=100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handson_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
